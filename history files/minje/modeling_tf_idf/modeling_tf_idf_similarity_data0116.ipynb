{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore') \n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "know_train = [pd.read_csv(path) for path in sorted(glob('../data_0112/train/*.csv'))]\n",
    "know_test = [pd.read_csv(path) for path in sorted(glob('../data_0112/test/*.csv'))]\n",
    "\n",
    "other_train = [pd.read_csv(path) for path in sorted(glob('../data_0105/train/*.csv'))]\n",
    "other_test = [pd.read_csv(path) for path in sorted(glob('../data_0105/test/*.csv'))]\n",
    "\n",
    "for num in range(4):\n",
    "    know_train[num]['text_response'] = other_train[num]['text_response']\n",
    "    know_train[num]['major'] = other_train[num]['major']\n",
    "    know_test[num]['text_response'] = other_test[num]['text_response']\n",
    "    know_test[num]['major'] = other_test[num]['major']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglishOrKorean(input_s):\n",
    "    k_count = 0\n",
    "    e_count = 0\n",
    "    for c in input_s:\n",
    "        if ord('가') <= ord(c) <= ord('힣'):\n",
    "            k_count+=1\n",
    "        elif ord('a') <= ord(c.lower()) <= ord('z'):\n",
    "            e_count+=1\n",
    "    return \"k\" if k_count>1 else \"e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체적인 오류 0값의 분포를 봅시다\n",
    "years = ['2017','2018','2019','2020']\n",
    "\n",
    "\n",
    "def fill_mean(x):\n",
    "    \n",
    "    filled_x = x.copy()\n",
    "    \n",
    "    zero_indice = filled_x.loc[filled_x==0].index\n",
    "    non_zero_indice = filled_x.loc[filled_x!=0].index\n",
    "    mean_values = round(filled_x.loc[non_zero_indice].mean(),0)\n",
    "    if len(zero_indice) == 0:\n",
    "        return x\n",
    "    else:\n",
    "        filled_x.loc[zero_indice] = mean_values\n",
    "        return filled_x\n",
    "\n",
    "def fill_mode(x):\n",
    "    filled_x = x.copy()\n",
    "    \n",
    "    zero_indice = filled_x.loc[filled_x==0].index\n",
    "    non_zero_indice = filled_x.loc[filled_x!=0].index\n",
    "    mode_values = round(filled_x.loc[non_zero_indice].mode(),0)[0]\n",
    "    if len(zero_indice) == 0:\n",
    "        return x\n",
    "    else:\n",
    "        filled_x.loc[zero_indice] = mode_values\n",
    "        return filled_x\n",
    "\n",
    "\n",
    "def data_imputation(skip_cols_year, numeric_pure_cols_year, train_data, test_data):\n",
    "\n",
    "    skip_txt_col_train = [col for col in train_data.columns if isEnglishOrKorean(col) == 'k'] + ['major','knowcode','idx','text_response','ubda_cnt']\n",
    "    error_cols_year_train = [col for col in train_data.columns if col not in skip_cols_year + skip_txt_col_train]\n",
    "    skip_txt_col_test = [col for col in test_data.columns if isEnglishOrKorean(col) == 'k'] + ['major','knowcode','idx','text_response','ubda_cnt']\n",
    "    error_cols_year_test = [col for col in test_data.columns if col not in skip_cols_year + skip_txt_col_test]\n",
    "    \n",
    "    mean_fill_cols_train = []\n",
    "    mode_fill_cols_train = []\n",
    "    mean_fill_cols_test = []\n",
    "    mode_fill_cols_test = []\n",
    "\n",
    "    for col in error_cols_year_train:\n",
    "        if col in numeric_pure_cols_year:\n",
    "            mean_fill_cols_train.append(col)\n",
    "        else:\n",
    "            mode_fill_cols_train.append(col)\n",
    "            \n",
    "    for col in error_cols_year_test:\n",
    "        if col in numeric_pure_cols_year:\n",
    "            mean_fill_cols_test.append(col)\n",
    "        else:\n",
    "            mode_fill_cols_test.append(col)\n",
    "            \n",
    "    for col in mean_fill_cols_train:\n",
    "        train_data[col] = fill_mean(train_data[col])\n",
    "        \n",
    "    for col in mode_fill_cols_train:\n",
    "        train_data[col] = fill_mode(train_data[col])\n",
    "        \n",
    "    for col in mean_fill_cols_test:\n",
    "        test_data[col] = fill_mode(test_data[col])\n",
    "        \n",
    "    for col in mode_fill_cols_test:\n",
    "        test_data[col] = fill_mode(test_data[col])\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "## 2017\n",
    "# 설문지에서 건너뛰어도 된다고 말한 문항\n",
    "skip_cols_2017 = ['aq1_2', 'aq2_2', 'aq3_2', 'aq4_2', 'aq5_2', 'aq6_2', 'aq7_2', 'aq8_2', 'aq9_2', 'aq10_2'\n",
    "                    ,'aq11_2', 'aq12_2', 'aq13_2', 'aq14_2', 'aq15_2', 'aq16_2', 'aq17_2', 'aq18_2', 'aq19_2', 'aq20_2'\n",
    "                    ,'aq21_2', 'aq22_2', 'aq23_2', 'aq24_2', 'aq25_2', 'aq26_2', 'aq27_2', 'aq28_2', 'aq29_2', 'aq30_2'\n",
    "                    ,'aq31_2', 'aq32_2', 'aq33_2', 'aq34_2', 'aq35_2', 'aq36_2', 'aq37_2', 'aq38_2', 'aq39_2', 'aq40_2'\n",
    "                    ,'aq41_2','bq5_1', 'bq40','bq41_1', 'bq41_2', 'bq41_3']\n",
    "numeric_pure_cols_2017 = ['bq23', 'bq37', 'bq41_1', 'bq41_2', 'bq41_3',]\n",
    "\n",
    "## 2018\n",
    "# 설문지에서 건너뛰어도 된다고 말한 문항\n",
    "skip_cols_2018 = ['bq5_1','bq25_1','bq39','bq40','bq41_1','bq41_2','bq41_3']\n",
    "numeric_pure_cols_2018 = ['bq21', 'bq36', 'bq40', 'bq41_1', 'bq41_2', 'bq41_3', ]\n",
    "\n",
    "## 2019\n",
    "# 설문지에서 건너뛰어도 된다고 말한 문항\n",
    "skip_cols_2019 = ['kq1_2', 'kq2_2', 'kq3_2', 'kq4_2', 'kq5_2', 'kq6_2', 'kq7_2', 'kq8_2', 'kq9_2', 'kq10_2'\n",
    "                ,'kq11_2', 'kq12_2', 'kq13_2', 'kq14_2', 'kq15_2','kq16_2', 'kq17_2', 'kq18_2', 'kq19_2','kq20_2'\n",
    "                ,'kq21_2', 'kq22_2', 'kq23_2', 'kq24_2', 'kq25_2','kq26_2', 'kq27_2' 'kq28_2', 'kq29_2', 'kq30_2'\n",
    "                ,'kq31_2', 'kq32_2','kq33_2','bq5_1','bq29','bq30','bq31_1','bq31_2','bq31_3'\n",
    "                ]\n",
    "numeric_pure_cols_2019 = ['bq26', 'bq30', 'bq31_1', 'bq31_2', 'bq31_3', ]\n",
    "\n",
    "## 2020\n",
    "# 설문지에서 건너뛰어도 된다고 말한 문항\n",
    "skip_cols_2020 = ['saq1_2', 'saq2_2', 'saq3_2', 'saq4_2', 'saq5_2','saq6_2', 'saq7_2', 'saq8_2', 'saq9_2', 'saq10_2'\n",
    "                 ,'saq11_2', 'saq12_2', 'saq13_2', 'saq14_2','saq15_2', 'saq16_2', 'saq17_2', 'saq18_2', 'saq19_2'\n",
    "                 ,'saq20_2', 'saq21_2', 'saq22_2', 'saq23_2', 'saq24_2', 'saq25_2', 'saq26_2', 'saq27_2', 'saq28_2', 'saq29_2'\n",
    "                 ,'saq30_2', 'saq31_2', 'saq32_2', 'saq33_2', 'saq34_2', 'saq35_2','saq36_2', 'saq37_2', 'saq38_2' \n",
    "                 , 'saq39_2', 'saq40_2', 'saq41_2', 'saq42_2',  'saq43_2', 'saq44_2' \n",
    "                 ,'bq5_1','bq28','bq29','bq30_1','bq30_2','bq30_3'\n",
    "                ]\n",
    "numeric_pure_cols_2020 = ['bq25', 'bq29', 'bq30_1', 'bq30_2', 'bq30_3', ]\n",
    "\n",
    "\n",
    "## 내보내기\n",
    "years = ['2017','2018','2019','2020']\n",
    "skip_cols_list = [skip_cols_2017, skip_cols_2018, skip_cols_2019, skip_cols_2020]\n",
    "numeric_pure_cols_list = [numeric_pure_cols_2017, numeric_pure_cols_2018, numeric_pure_cols_2019,numeric_pure_cols_2020]\n",
    "\n",
    "for idx in range(4):\n",
    "    train_data, test_data = data_imputation(skip_cols_list[idx], numeric_pure_cols_list[idx], know_train[idx], know_test[idx])\n",
    "    know_train[idx] = train_data\n",
    "    know_test[idx] = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 : 같은 연도인 train_set, test_set으로 sim_df 만들기\n",
    "# input\n",
    "def get_tf_idf_sim_mat(train_data, test_data):\n",
    "\n",
    "    total_nouns_list = []\n",
    "\n",
    "    doc_nouns_list_train = [doc for doc in (train_data['text_response'] +' '+ train_data['major'])]\n",
    "    doc_nouns_list_test = [doc for doc in (test_data['text_response'] +' '+ test_data['major'])]\n",
    "\n",
    "    total_nouns_list.extend(doc_nouns_list_train)\n",
    "    total_nouns_list.extend(doc_nouns_list_test)\n",
    "\n",
    "    stopwords = ['없다', '공란', '0']\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=1,stop_words=stopwords)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(total_nouns_list)\n",
    "    doc_nouns_similarities = (tfidf_matrix * tfidf_matrix.T)\n",
    "    sim_df = pd.DataFrame(doc_nouns_similarities.toarray())\n",
    "    \n",
    "    return sim_df\n",
    "\n",
    "# STEP 2 : test_set의 특정 레코드와 가장 비슷한 레코드를 train_set에서 찾고 그 레코드의 knowcode를 결과값으로 내놓기\n",
    "def sim_pred(idx, sim_df, threshold, train_data):\n",
    "    \n",
    "    train_len = train_data.shape[0]\n",
    "    \n",
    "    test_set_record = idx + train_len   # sim_df에서 test_set의 index를 계산\n",
    "    sim_rank_series = sim_df.loc[test_set_record].sort_values(ascending=False) \n",
    "    filtered_sim_rank_series = sim_rank_series.loc[sim_rank_series.index < train_len]  # test_set_record와 가장 비슷한 train_set_record를 계산\n",
    "\n",
    "    target_index = list(filtered_sim_rank_series.head(1).index)[0]\n",
    "    target_similarity = list(filtered_sim_rank_series.head(1).values)[0]\n",
    "\n",
    "    if target_similarity > threshold:   # 조건을 만족한다면 유사한 train_set의 knowcode와 같은 것으로 예측\n",
    "        pred = train_data.loc[target_index,'knowcode']\n",
    "        \n",
    "    else:   # 조건을 만족하지 않는다면 건너뛰었다는 의미로 0을 반환\n",
    "        pred = 0\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:37<00:00,  9.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# model을 먼저 도출하자\n",
    "years = ['2017','2018','2019','2020']\n",
    "train_data = {}\n",
    "\n",
    "for year, df in zip(years, know_train):\n",
    "    train_data[year] = {'X': df.drop(['text_response','idx','major','knowcode'], axis=1),\n",
    "                        'y': df['knowcode']}\n",
    "    \n",
    "RANDOM_STATE = 42\n",
    "et_models = {}\n",
    "\n",
    "for year in tqdm(years):\n",
    "    model = ExtraTreesClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=8)\n",
    "    model.fit(train_data[year]['X'], train_data[year]['y'])\n",
    "    et_models[year] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data 준비하기\n",
    "test_data = {}\n",
    "\n",
    "for year, df in zip(years, know_test):\n",
    "    train_columns = train_data[year]['X'].columns\n",
    "    test_data[year] =  {'X': df[train_columns]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:33, 23.50s/it]\n"
     ]
    }
   ],
   "source": [
    "## 예측을 진행해보자\n",
    "\n",
    "sim_predicts = [] \n",
    "threshold = 0.4\n",
    "\n",
    "for year, num in tqdm(zip(years,range(4))):\n",
    "    sim_df = get_tf_idf_sim_mat(know_train[num], know_test[num])    # sim_df를 먼저 구해주고\n",
    "    sim_predict = []\n",
    "    \n",
    "    for idx in range(test_data[year]['X'].shape[0]):\n",
    "        pred = sim_pred(idx, sim_df, threshold, know_train[num])\n",
    "        sim_predict.append(pred)\n",
    "\n",
    "    sim_predicts.extend(sim_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data_0103/sample_submission.csv') # sample submission 불러오기\n",
    "submission['knowcode'] = sim_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "non_sim_predicts = []\n",
    "idx_start = 0\n",
    "idx_end = 0\n",
    "\n",
    "final_submission = pd.DataFrame()\n",
    "for year in tqdm(years):\n",
    "    test_len = test_data[year]['X'].shape[0] - 1\n",
    "    idx_end = idx_start + test_len\n",
    "    submission_year = submission.loc[idx_start:idx_end,:].reset_index(drop=True)\n",
    "    idx_start = idx_start + test_len + 1\n",
    "    \n",
    "    zero_indice = submission_year[submission_year['knowcode']==0].index     # 이전 예측에서 0으로 지정했던 데이터의 인덱스\n",
    "    \n",
    "    test_data_zero = test_data[year]['X'].loc[zero_indice,:]    # 그것들만 따로 추려서 model에 넣어 예측해주고\n",
    "    pred = et_models[year].predict(test_data_zero)\n",
    "    \n",
    "    submission_year.loc[zero_indice,'knowcode'] = pred      # submission에 채워줍니다\n",
    "    final_submission = pd.concat([final_submission, submission_year])\n",
    "    \n",
    "final_submission = final_submission.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.to_csv('et_data0112_tf_idf_matrix_sim_ver5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
