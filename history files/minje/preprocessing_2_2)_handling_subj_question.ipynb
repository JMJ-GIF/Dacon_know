{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling subjective question_ver2\n",
    ": **주관식으로 답변한 문항들에 대해서 명사 추출을 진행하고, text_response 열로 합쳐줍니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트하기\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "# 데이터가 담겨있는 path로 설정하기\n",
    "mypath = '../data_0104'\n",
    "os.chdir(mypath)\n",
    "\n",
    "warnings.filterwarnings(action='ignore') \n",
    "pd.options.display.max_columns=None\n",
    "know_train = [pd.read_csv(path) for path in sorted(glob('./train/*.csv'))]\n",
    "know_test = [pd.read_csv(path) for path in sorted(glob('./test/*.csv'))]\n",
    "pdf_list =  [pd.read_csv(path) for path in sorted(glob('../data_pdf_description/*.csv'))]\n",
    "\n",
    "# Stopwords가 담겨있는 path를 설정하기\n",
    "Stopwords_path = '../minje/stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------#\n",
    "#                                                Input global variable                                                     #\n",
    "# -------------------------------------------------------------------------------------------------------------------------#\n",
    "# 1) years(4개의 연도)\n",
    "# type : list\n",
    "\n",
    "years = ['2017','2018','2019','2020']\n",
    "\n",
    "\n",
    "# 2) text_dict (연도별로 str 처리된 문항들) \n",
    "# type : dict\n",
    "\n",
    "text_2017_question = ['bq4_1a','bq4_1b','bq4_1c','bq5_2','bq19_1','bq31','bq34']\n",
    "text_2018_question = ['bq4_1a','bq4_1b','bq4_1c','bq5_2','bq28_1','bq30','bq33']\n",
    "text_2019_question = ['bq4_1a','bq4_1b','bq4_1c','bq5_2','bq18_10','bq20_1','bq24']\n",
    "text_2020_question = ['bq4_1a','bq4_1b','bq4_1c','bq5_2','bq18_10','bq20_1']\n",
    "\n",
    "whole_list = [text_2017_question, text_2018_question, text_2019_question, text_2020_question]\n",
    "\n",
    "text_dict = {}\n",
    "for year, lst in zip(years, whole_list):\n",
    "    text_dict[year] = lst\n",
    "       \n",
    "        \n",
    "# 3) stopwords(불용어 목록)\n",
    "# type : list\n",
    " \n",
    "# 불용어 목록 읽어오기(인터넷에서 일단 다운로드 받았습니다)\n",
    "stopwords = []\n",
    "f = open(Stopwords_path, 'rt', encoding='UTF8')\n",
    "line = f.readline()\n",
    "while line:\n",
    "    line = f.readline().replace('\\n',\"\")\n",
    "    stopwords.append(line)\n",
    "f.close()\n",
    "\n",
    "# stopwords에서 공백을 제거합시다\n",
    "remove_set = {''}\n",
    "stopwords = [i for i in stopwords if i not in remove_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text_data(year,know_data,preprocessing_question,exception_question):\n",
    "    ''' \n",
    "    데이터 내에 존재하는 text 문항들을 konlpy를 통해 명사화 하고, 하나의 칼럼으로 합쳐줍니다.\n",
    "    또한 text데이터 내에 존재하는 '0'도 모두 제거해줍니다.\n",
    "    \n",
    "    Parameters\n",
    "    year(str)                    : 전처리 대상이 되는 데이터의 연도\n",
    "    know_data(dataframe)         : 전처리 대상이 되는 데이터프레임\n",
    "    preprocessing_question(list) : 약술형, 서술형으로 명사 추출 과정이 필요한 문항을 명시(이 리스트에 들어간 문항만 명사 추출)\n",
    "    exception_question(list)     : 명사추출과 칼럼 병합에 있어 예외를 두고 싶은 문항(전처리 하지 않음)\n",
    "    \n",
    "    Returns\n",
    "    know_data(dataframe) : 전처리가 완료된 데이터프레임\n",
    "    '''\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------#\n",
    "    #                               약술형과 서술형 문항에 대해서 명사를 추출하고 따로 전처리 해주기                               #\n",
    "    # -------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    ## konlpy를 이용한 lemmatization - 조사나 어미같이 단어에서 의미론적으로 필요없는 부분 정리하기\n",
    "    for question in preprocessing_question:\n",
    "        okt = Okt()\n",
    "        target_question = [' '.join(okt.nouns(doc)) for doc in know_data[question]] # 명사를 뽑기\n",
    "        \n",
    "        # [(stopwords에 포함되어 있고) and (길이가 1인 명사)]는 제외\n",
    "        doc_nouns_list = target_question\n",
    "        new_doc_nouns_list = []\n",
    "\n",
    "        for idx in range(len(doc_nouns_list)):\n",
    "            doc_list = doc_nouns_list[idx].split()\n",
    "            new_words = ''\n",
    "            for word in doc_list:\n",
    "                if word not in set(stopwords): # 단어가 불용어 사전에 존재하지 않고\n",
    "                    if len(word) > 1: # 길이가 1인 경우는 필터링합니다\n",
    "                        new_words += word\n",
    "                        new_words += ' '\n",
    "            new_doc_nouns_list.append(new_words[:-1])\n",
    "            \n",
    "        # 모두 필터링되어 아무것도 나타나지 않는 단어는 0으로 치환합니다\n",
    "        result_list = []\n",
    "        for word in new_doc_nouns_list:\n",
    "            if word == '':\n",
    "                result_list.append('0')\n",
    "            else:\n",
    "                result_list.append(word)\n",
    "                \n",
    "        # 전처리 과정을 모두 거친 데이터를 반환합니다\n",
    "        know_data[question] = result_list\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------#\n",
    "    #                                           다른 문항들과 명사추출한 문항들을 합쳐주기                                        #\n",
    "    # -------------------------------------------------------------------------------------------------------------------------#\n",
    "       \n",
    "    # preprocessing_question을 제외한 나머지 text 칼럼들은 전부 합쳐줍니다\n",
    "    text_agg_col = pd.DataFrame(index=range(know_data.shape[0]))\n",
    "\n",
    "    # 맨 처음 열은 미리 넣어줍시다\n",
    "    idx = 0\n",
    "    while True:\n",
    "        if text_agg_col.shape[1] != 0: # 하나라도 데이터프레임에 들어가면 break\n",
    "            break\n",
    "        else:\n",
    "            first_col = text_dict[year][idx] \n",
    "            if first_col not in exception_question: # 넣고자 하는 열이 예외문항이 아닌경우에만 병합\n",
    "                text_agg_col['text_agg_col'] = know_data[first_col]\n",
    "            else: # 아니면 다른 열을 탐색\n",
    "                idx += 1\n",
    "\n",
    "    # 넣은 열을 제외한 agg_col\n",
    "    remove_set = {first_col}\n",
    "    target_agg_col = [i for i in text_dict[year] if i not in remove_set]\n",
    "\n",
    "    # 예외로 빼는 문항을 제외하고 모든 주관식 문항을 하나의 열에 합칩니다\n",
    "    for text_col in target_agg_col:\n",
    "        if text_col in exception_question: # 예외로 다룰 문항은 병합하면 안되므로 따로 빼주기\n",
    "            continue\n",
    "        text_agg_col['text_agg_col'] = text_agg_col['text_agg_col'] + ' ' + know_data[text_col]\n",
    "        \n",
    "    # 합친 문항을 원래 데이터에 넣어줍니다\n",
    "    know_data['text_response'] = text_agg_col['text_agg_col']\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------#\n",
    "    #                                 최종적으로 구한 text_response열에 대해서 전처리하기                                        #\n",
    "    # -------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    # 최종적으로 구한 text_response 열에 대해 마지막 전처리를 해줍니다\n",
    "    # 1) text_response열에서 '0'인 데이터는 모두 제거해줍니다\n",
    "    for idx in range(len(know_data['text_response'])):\n",
    "        doc_list = know_data['text_response'][idx].split()\n",
    "        remove_set = {'0'}\n",
    "        doc_list = [i for i in doc_list if i not in remove_set]\n",
    "        doc_string = ' '.join(doc_list)\n",
    "        know_data.loc[idx,'text_response'] = doc_string\n",
    "\n",
    "    # 2) text_response열에서 특수문자는 모두 제거해줍니다\n",
    "    know_data[\"text_response\"] = know_data[\"text_response\"].str.replace(pat=r'[^\\w]', repl=r\" \", regex=True) # 모든 특수문자를 제거합니다\n",
    "    \n",
    "    # 3) text_response열에서 nan값은 공란으로 대체합니다\n",
    "    know_data[\"text_response\"] = know_data[\"text_response\"].fillna('공란') # 생각보다 주관식을 작성하지 않은 사람들이 많습니다. 그런 유저는 공란으로 처리합니다\n",
    "\n",
    "    # 4) text_response열에서 단어의 길이가 1인 경우 제외하고, ''으로 표현된 단어도 공란으로 대체합니다\n",
    "    for idx in range(len(know_data['text_response'])):\n",
    "        doc_list = know_data['text_response'][idx].split()\n",
    "        new_words = ''\n",
    "        for word in doc_list:\n",
    "            if word.isdigit() == False: # 숫자로만 되어있는 단어가 아니어야 함\n",
    "                if len(word) > 1: # 길이가 1인 경우 필터링하기\n",
    "                    new_words += word\n",
    "                    new_words += ' '\n",
    "        if new_words[:-1] == '': # 모든 필터링을 거치고 데이터가 없을 경우도 공란으로 처리합니다\n",
    "            know_data.loc[idx,'text_response'] = '공란' \n",
    "        else:\n",
    "            know_data.loc[idx,'text_response'] = new_words[:-1]\n",
    "    \n",
    "    # 합치기 이전 문항들은 모두 드랍합니다\n",
    "    know_data = know_data.drop(list(set(text_dict[year]) - set(exception_question)), axis=1)\n",
    "    \n",
    "    return know_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f7f4664b1d4a77ab641c2a224e80a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 : train_set 진행중\n",
      "2018 : train_set 진행중\n",
      "2019 : train_set 진행중\n",
      "2020 : train_set 진행중\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e61ff82d7b9411989eb406c3e372019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 : test_set 진행중\n",
      "2018 : test_set 진행중\n",
      "2019 : test_set 진행중\n",
      "2020 : test_set 진행중\n"
     ]
    }
   ],
   "source": [
    "preprocessing_question_2017 = ['bq19_1', 'bq31']\n",
    "preprocessing_question_2018 = ['bq28_1', 'bq29']\n",
    "preprocessing_question_2019 = ['bq18_10','bq20_1']\n",
    "preprocessing_question_2020 = ['bq18_10','bq20_1']\n",
    "\n",
    "whole_preprocessing_question = [preprocessing_question_2017, preprocessing_question_2018, preprocessing_question_2019, preprocessing_question_2020]\n",
    "\n",
    "exception_question_2017 = ['bq30','bq32','bq33','bq38_1']\n",
    "exception_question_2018 = ['bq29','bq31','bq32','bq37_1']\n",
    "exception_question_2019 = ['bq22','bq23','bq27_1']\n",
    "exception_question_2020 = ['bq26_1']\n",
    "\n",
    "whole_exception_question = [exception_question_2017, exception_question_2018, exception_question_2019, exception_question_2020]\n",
    "\n",
    "\n",
    "# 모든 데이터 셋에 대해 전처리를 진행합니다\n",
    "idx = 0\n",
    "for year, df, pre_q, exc_q in tqdm_notebook(zip(years, know_train, whole_preprocessing_question, whole_exception_question)):\n",
    "    print(year,':','train_set 진행중')\n",
    "    new_data = preprocessing_text_data(year,df,pre_q,exc_q)\n",
    "    \n",
    "    if year == '2017':\n",
    "        new_data['sim_job'] = new_data['bq30']\n",
    "        new_data['bef_job'] = new_data['bq32']\n",
    "        new_data['able_job'] = new_data['bq33']\n",
    "        new_data['major'] = new_data['bq38_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "        \n",
    "    elif year == '2018':\n",
    "        new_data['sim_job'] = new_data['bq29']\n",
    "        new_data['bef_job'] = new_data['bq31']\n",
    "        new_data['able_job'] = new_data['bq32']\n",
    "        new_data['major'] = new_data['bq37_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "\n",
    "    elif year == '2019':\n",
    "        new_data['bef_job'] = new_data['bq22']\n",
    "        new_data['able_job'] = new_data['bq23']\n",
    "        new_data['major'] = new_data['bq27_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "   \n",
    "    elif year == '2020':\n",
    "        new_data['major'] = new_data['bq26_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "    \n",
    "    new_data = pd.merge(new_data, pdf_list[idx],on='knowcode',how='left').fillna('0')\n",
    "    new_data.to_csv('KNOW_{0}.csv'.format(year), index=False)\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "for year, df, pre_q, exc_q in tqdm_notebook(zip(years, know_test, whole_preprocessing_question, whole_exception_question)):\n",
    "    print(year,':','test_set 진행중')\n",
    "    new_data = preprocessing_text_data(year,df,pre_q,exc_q)\n",
    "    \n",
    "    if year == '2017':\n",
    "        new_data['sim_job'] = new_data['bq30']\n",
    "        new_data['bef_job'] = new_data['bq32']\n",
    "        new_data['able_job'] = new_data['bq33']\n",
    "        new_data['major'] = new_data['bq38_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "        \n",
    "    elif year == '2018':\n",
    "        new_data['sim_job'] = new_data['bq29']\n",
    "        new_data['bef_job'] = new_data['bq31']\n",
    "        new_data['able_job'] = new_data['bq32']\n",
    "        new_data['major'] = new_data['bq37_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "\n",
    "    elif year == '2019':\n",
    "        new_data['bef_job'] = new_data['bq22']\n",
    "        new_data['able_job'] = new_data['bq23']\n",
    "        new_data['major'] = new_data['bq27_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "   \n",
    "    elif year == '2020':\n",
    "        new_data['major'] = new_data['bq26_1']\n",
    "        new_data = new_data.drop(exc_q,axis=1)\n",
    "        \n",
    "    new_data.to_csv('KNOW_{0}_test.csv'.format(year), index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
