{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# et_tf_idf_matrix_pca_Model\n",
    "\n",
    "1. 데이터  \n",
    "    * data_0119를 사용함  \n",
    "    * data_0105에 있는 text_response열을 추가함\n",
    "    * 기타 categorical 열에 대해 라벨인코딩 진행\n",
    "# \n",
    "2. pca after getting tf-idf matrix  \n",
    "    * STEP 1 : text_response열에 대한 tf_idf_matrix를 구함  \n",
    "    * STEP 2 : tf_idf_matrix를 2차원으로 pca해서 train_data에 append 함\n",
    "#      \n",
    "3. fitting & Prediction  \n",
    "    * et_300으로 data를 학습하고, 예측함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 import & 데이터 로딩 & 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n",
      "HEHE\n",
      "2018\n",
      "HEHE\n",
      "2019\n",
      "HEHE\n",
      "2020\n",
      "HEHE\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "warnings.filterwarnings(action='ignore') \n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "know_train = [pd.read_csv(path) for path in sorted(glob('../data_0119/train/*.csv'))]\n",
    "know_test = [pd.read_csv(path) for path in sorted(glob('../data_0119/test/*.csv'))]\n",
    "\n",
    "other_train = [pd.read_csv(path) for path in sorted(glob('../data_0105/train/*.csv'))]\n",
    "other_test = [pd.read_csv(path) for path in sorted(glob('../data_0105/test/*.csv'))]\n",
    "\n",
    "# data_0119에 text_response 추가해주기\n",
    "for num in range(4):\n",
    "    know_train[num]['text_response'] = other_train[num]['text_response']\n",
    "    know_test[num]['text_response'] = other_test[num]['text_response']\n",
    "    know_train[num]['text_response'] = know_train[num]['text_response'] + ' ' + know_train[num]['major']\n",
    "    know_test[num]['text_response'] = know_test[num]['text_response'] + ' ' + know_test[num]['major']\n",
    "\n",
    "text_info_cols = ['text_response']\n",
    "\n",
    "## train\n",
    "for i in range(4):\n",
    "    for text_info_col in text_info_cols:\n",
    "        know_train[i].loc[know_train[i][text_info_col]=='없다', text_info_col] = ''\n",
    "        know_train[i].loc[know_train[i][text_info_col]=='없음', text_info_col] = ''\n",
    "        know_train[i].loc[know_train[i][text_info_col]=='0', text_info_col] = ''\n",
    "        know_train[i].loc[know_train[i][text_info_col]=='무', text_info_col] = ''\n",
    "        know_train[i].loc[know_train[i][text_info_col]=='모름', text_info_col] = ''\n",
    "        know_train[i].loc[know_train[i][text_info_col]=='공란', text_info_col] = ''\n",
    "\n",
    "## test  \n",
    "for i in range(4):\n",
    "    for text_info_col in text_info_cols:\n",
    "        know_test[i].loc[know_test[i][text_info_col]=='없다', text_info_col] = ''\n",
    "        know_test[i].loc[know_test[i][text_info_col]=='없음', text_info_col] = ''\n",
    "        know_test[i].loc[know_test[i][text_info_col]=='0', text_info_col] = ''\n",
    "        know_test[i].loc[know_test[i][text_info_col]=='무', text_info_col] = ''\n",
    "        know_test[i].loc[know_test[i][text_info_col]=='모름', text_info_col] = ''\n",
    "        know_test[i].loc[know_test[i][text_info_col]=='공란', text_info_col] = ''\n",
    "\n",
    "# encode train data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "years = ['2017', '2018', '2019', '2020']\n",
    "\n",
    "year_encoder = {}\n",
    "\n",
    "for year, df in zip(years, know_train):\n",
    "    print(year)\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in df.columns.difference(['text_response']):\n",
    "        if col == 'idx':\n",
    "            print(\"HEHE\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df[col] = df[col].map(int)\n",
    "        except:\n",
    "            encoder = LabelEncoder()\n",
    "            df[col] = df[col].map(str)\n",
    "            df[col] = encoder.fit_transform(df[col])\n",
    "            encoders[col] = encoder\n",
    "            \n",
    "            \n",
    "    year_encoder[year] = encoders\n",
    "\n",
    "# encode test data\n",
    "years = ['2017', '2018', '2019', '2020']\n",
    "\n",
    "for year, df in zip(years, know_test):\n",
    "    print(year)\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in df.columns.difference(['text_response']):\n",
    "        \n",
    "        try:\n",
    "            df[col] = df[col].map(int)\n",
    "        except:\n",
    "            encoder = year_encoder[year][col]\n",
    "            df[col] = df[col].map(str)\n",
    "            category_map = {category: idx for idx, category in enumerate(encoder.classes_)}\n",
    "            df[col] = df[col].apply(lambda x: category_map[x] if x in category_map else -1) # train set에서 보지못한 카테고리변수 -1(UNK) 처리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pca after getting tf-idf matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_df(know_data):\n",
    "    \n",
    "    doc_nouns_list = [doc for doc in (know_data['text_response'])]\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "    tfidf_matrix = pd.DataFrame(tfidf_vectorizer.fit_transform(doc_nouns_list).toarray())\n",
    "    \n",
    "    return tfidf_matrix\n",
    "\n",
    "def use_pca(preprocessed_data, n_component):\n",
    "\n",
    "    # 평균이 0이 되도록 조정\n",
    "    data_scaled = StandardScaler().fit_transform(preprocessed_data)\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_component)\n",
    "    \n",
    "    data_pca = pd.DataFrame(pca.fit_transform(data_scaled), columns = range(n_component))\n",
    "    \n",
    "    return data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 준비\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:39,  9.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 완료\n",
      "--------\n",
      "test_data 준비\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:43, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 데이터 준비\n",
    "\n",
    "years = ['2017','2018','2019','2020']\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "n_components = {}\n",
    "\n",
    "print('train data 준비')\n",
    "for year, df in tqdm(zip(years, know_train)):\n",
    "    \n",
    "    n_component = 2\n",
    "    \n",
    "    tf_idf_matrix = get_tf_idf_df(df)\n",
    "    \n",
    "    tf_idf_pca = use_pca(tf_idf_matrix, n_component)\n",
    "    \n",
    "    train_pca = pd.concat([df,tf_idf_pca],axis=1).drop(['idx','knowcode','text_response','description'], axis = 1)\n",
    "    \n",
    "    train_data[year] = {'X': train_pca, # idx 제외\n",
    "                        'y': df['knowcode']}\n",
    "    \n",
    "    n_components[year] = n_component\n",
    "    \n",
    "print('train data 완료')\n",
    "print('--------')\n",
    "print('test_data 준비')    \n",
    "for year, df in tqdm(zip(years, know_test)):\n",
    "    \n",
    "    n_component = n_components[year]\n",
    "    \n",
    "    tf_idf_matrix = get_tf_idf_df(df)\n",
    "    \n",
    "    tf_idf_pca = use_pca(tf_idf_matrix, n_component)\n",
    "    \n",
    "    test_pca = pd.concat([df,tf_idf_pca],axis=1).drop(['idx','text_response'], axis = 1)\n",
    "    \n",
    "    test_data[year] = test_pca\n",
    "\n",
    "print('test_data 완료') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitting & Prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [07:58<00:00, 119.73s/it]\n"
     ]
    }
   ],
   "source": [
    "## 모델 학습\n",
    "RANDOM_STATE = 42    \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_predicts = [] \n",
    "for year in tqdm(years):\n",
    "    # train\n",
    "    model = ExtraTreesClassifier(n_estimators=400, random_state=RANDOM_STATE, n_jobs=8)\n",
    "    model.fit(train_data[year]['X'].iloc[:, :], train_data[year]['y'])\n",
    "\n",
    "    # predict\n",
    "    pred = model.predict(test_data[year])\n",
    "    et_predicts.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data_0103/sample_submission.csv') # sample submission 불러오기\n",
    "submission['knowcode'] = et_predicts\n",
    "submission.to_csv('et_400_data0119_append_tf_idf_matrix_pca.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
